# Microproyecto MAIA â€” AnÃ¡lisis de documentos cientÃ­ficos en espaÃ±ol

**Tema 1 â€“ 2026** Â· Grupo FLAG-TICsW Â· Universidad de los Andes

Este repositorio corresponde a un microproyecto MAIA cuyo objetivo es desarrollar una soluciÃ³n computacional para el anÃ¡lisis automÃ¡tico de documentos cientÃ­ficos en espaÃ±ol, abordando (i) la segmentaciÃ³n y clasificaciÃ³n retÃ³rica y (ii) la extracciÃ³n de contribuciones cientÃ­ficas.

## PropÃ³sito del repositorio

Este proyecto implementa:

- PreparaciÃ³n y curadurÃ­a de un corpus cientÃ­fico en espaÃ±ol.
- Modelos para segmentaciÃ³n y clasificaciÃ³n retÃ³rica.
- Modelos para detecciÃ³n de contribuciones cientÃ­ficas.
- EvaluaciÃ³n comparativa entre modelos entrenados y modelos de lenguaje.
- AplicaciÃ³n web interactiva para visualizaciÃ³n de resultados.
- Scripts reproducibles para experimentaciÃ³n y anÃ¡lisis de resultados.

El enfoque es acadÃ©mico y experimental, orientado a entregar una soluciÃ³n funcional y evaluable.

## Tareas

| Tarea | DescripciÃ³n | Labels / Tipos |
|-------|-------------|----------------|
| **Tarea 1** | SegmentaciÃ³n retÃ³rica | INTRO, BACK, METH, RES, DISC, CONTR, LIM, CONC |
| **Tarea 2** | DetecciÃ³n de contribuciones (binaria) | is_contribution = true/false |

## Estructura del proyecto

```
.
â”œâ”€â”€ app/                           # ğŸ–¥ï¸ AplicaciÃ³n web (frontend + backend)
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ main.py                # Entry point Flask
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py        # POST /api/analyze
â”‚   â”‚   â”‚   â””â”€â”€ comparison.py      # GET /api/compare/<id>
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ segmentation.py    # Tarea 1 (mock â†’ real)
â”‚   â”‚   â”‚   â”œâ”€â”€ contributions.py   # Tarea 2 (mock â†’ real)
â”‚   â”‚   â”‚   â””â”€â”€ models.py          # ConfiguraciÃ³n de modelos
â”‚   â”‚   â”œâ”€â”€ mock_data/             # JSONs de referencia
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ frontend/
â”‚       â”œâ”€â”€ index.html             # Vista 1: Entrada de texto
â”‚       â”œâ”€â”€ segmentation.html      # Vista 2: SegmentaciÃ³n retÃ³rica
â”‚       â”œâ”€â”€ contributions.html     # Vista 3: Contribuciones
â”‚       â”œâ”€â”€ comparison.html        # Vista 4: ComparaciÃ³n de modelos
â”‚       â”œâ”€â”€ css/styles.css
â”‚       â””â”€â”€ js/
â”‚           â”œâ”€â”€ app.js
â”‚           â”œâ”€â”€ api.js
â”‚           â””â”€â”€ charts.js
â”‚
â”œâ”€â”€ datos/                         # ğŸ“Š Datos del proyecto
â”‚   â””â”€â”€ core/                      # Corpus cientÃ­fico crudo (CORE)
â”‚
â”œâ”€â”€ src/                           # ğŸ”§ CÃ³digo fuente principal
â”‚   â”œâ”€â”€ preprocessing/             # Limpieza, normalizaciÃ³n y segmentaciÃ³n
â”‚   â”œâ”€â”€ task1_rhetorical/          # SegmentaciÃ³n y clasificaciÃ³n retÃ³rica
â”‚   â”œâ”€â”€ task2_contributions/       # ExtracciÃ³n de contribuciones cientÃ­ficas
â”‚   â”œâ”€â”€ models/                    # DefiniciÃ³n y carga de modelos
â”‚   â””â”€â”€ utils/                     # Funciones auxiliares comunes
â”‚
â”œâ”€â”€ experiments/                   # ğŸ§ª Experimentos y configuraciones
â”‚   â”œâ”€â”€ task1/                     # Experimentos de clasificaciÃ³n retÃ³rica
â”‚   â””â”€â”€ task2/                     # Experimentos de extracciÃ³n de contribuciones
â”‚
â”œâ”€â”€ evaluation/                    # ğŸ“ˆ EvaluaciÃ³n y anÃ¡lisis de resultados
â”‚   â”œâ”€â”€ metrics/                   # MÃ©tricas cuantitativas
â”‚   â””â”€â”€ error_analysis/            # AnÃ¡lisis cualitativo de errores
â”‚
â”œâ”€â”€ notebooks/                     # ğŸ““ AnÃ¡lisis exploratorio y pruebas
â”œâ”€â”€ artifacts/                     # Artefactos generados
â”œâ”€â”€ configs/                       # Configuraciones de modelos y experimentos
â”œâ”€â”€ data_lake/scripts/             # Scripts de data lake
â”‚
â”œâ”€â”€ .dvc/                          # ConfiguraciÃ³n DVC
â”œâ”€â”€ .dvcignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ datos.dvc                      # Tracking DVC del corpus
â”œâ”€â”€ Propuesta_Proyecto_PLN_FLAG.pdf
â””â”€â”€ README.md
```

## DescripciÃ³n de componentes

| Carpeta | DescripciÃ³n |
|---------|-------------|
| `app/` | AplicaciÃ³n web con backend Flask y frontend vanilla. Interfaz para analizar textos y comparar modelos. |
| `datos/` | Corpus de documentos cientÃ­ficos en espaÃ±ol (CORE API). |
| `src/` | LÃ³gica central: preprocesamiento, clasificaciÃ³n retÃ³rica, detecciÃ³n de contribuciones. |
| `experiments/` | Scripts para ejecutar experimentos controlados y comparables. |
| `evaluation/` | CÃ¡lculo de mÃ©tricas, matrices de confusiÃ³n y anÃ¡lisis de errores. |
| `notebooks/` | ExploraciÃ³n de datos, pruebas de modelos y anÃ¡lisis intermedios. |

## Alcance

Este repositorio cubre todo el flujo de soluciÃ³n:

- Desde documentos cientÃ­ficos crudos â†’ hasta resultados evaluados y comparables
- Incluye una aplicaciÃ³n web para visualizaciÃ³n interactiva de ambas tareas

---

## CÃ³mo lanzar el proyecto

### 1. Configurar variables de entorno (recomendado: `.env`)

El backend carga automÃ¡ticamente un archivo `.env` (si existe) al arrancar.

Variables tÃ­picas:

```bash
# Flask
PORT=5000
DEBUG=1
LOG_LEVEL=INFO  # DEBUG para mÃ¡s detalle

# Gemini (model="api")
GEMINI_API_KEY="..."
GEMINI_MODEL="gemini-2.5-flash"
GEMINI_TIMEOUT_S=60
GEMINI_STRUCTURED_OUTPUT=1

# Ollama (model="llm")
OLLAMA_BASE_URL="http://localhost:11434"
LOCAL_LLM_VARIANT=small   # small (~3B) | large (8B)
LOCAL_LLM_MODEL=""        # opcional: override del tag

# Cache local (evita re-descargas de modelos)
MODEL_CACHE_DIR=artifacts/model_cache
```

### 2. Configurar AWS (para descargar modelos desde S3)

```bash
aws configure
```

Verificar que quedaron activas:

```bash
aws sts get-caller-identity
```

### 3. Descargar los datos con DVC

Desde la raÃ­z del repositorio:

```bash
dvc pull
```

### 4. Ejecutar la aplicaciÃ³n web

```bash
cd app/backend

# Crear entorno virtual (recomendado)
python -m venv venv
source venv/bin/activate      # macOS/Linux
# venv\Scripts\activate       # Windows

# Instalar dependencias
pip install -r requirements.txt

# Ejecutar el servidor
python main.py
# â†’ Servidor en http://localhost:5000
```

Abre http://localhost:5000 en tu navegador.

---

## AplicaciÃ³n web â€” SciText-ES

### Vistas

1. **Entrada** (`/`) â€” Pega el texto, selecciona modelo y tareas
2. **SegmentaciÃ³n** (`/segmentation.html`) â€” PÃ¡rrafos clasificados con colores y confianza
3. **Contribuciones** (`/contributions.html`) â€” Fragmentos con aportes identificados
4. **ComparaciÃ³n** (`/comparison.html`) â€” MÃ©tricas F1/PrecisiÃ³n/Recall/Latencia de los 3 modelos

### API

**`POST /api/analyze`**
```json
{
  "text": "Texto del artÃ­culoâ€¦",
  "model": "encoder | llm | api",
  "tasks": ["segmentation", "contributions"],
  "encoder_variant": "roberta | scibert"
}
```
Devuelve segmentos etiquetados y fragmentos con contribuciones.

**`GET /api/compare/<analysis_id>`**

Devuelve mÃ©tricas comparativas de los 3 modelos para el texto analizado.

### Swagger / OpenAPI

La app expone documentaciÃ³n interactiva:

- UI: `http://localhost:5000/apidocs/`
- Spec JSON: `http://localhost:5000/apispec_1.json`

Para exportar el spec:
```bash
curl -s "http://localhost:5000/apispec_1.json" > apispec_1.json
```

### Variables de entorno

```bash
PORT=5000       # Puerto del servidor (por defecto: 5000)
DEBUG=1         # Modo debug de Flask (por defecto: 1)
```

### Modelos encoder (Task1 + Task2) vÃ­a MLflow (S3)

Para evitar commitear archivos grandes (por ejemplo `model.safetensors`) en GitHub, puedes subir los modelos a MLflow y referenciarlos por URI `runs:/...`.

**Cache local (evita re-descargas):**
```bash
MODEL_CACHE_DIR=artifacts/model_cache
```

**Task1 (segmentaciÃ³n retÃ³rica):**
```bash
TASK1_ENCODER_ROBERTA_MLFLOW_MODEL_URI="runs:/<run_id>/hf_model"
TASK1_ENCODER_SCIBERT_MLFLOW_MODEL_URI="runs:/<run_id>/hf_model"
```

**Task2 (contribuciones binario):**
```bash
TASK2_ENCODER_ROBERTA_MLFLOW_MODEL_URI="runs:/<run_id>/hf_model"
TASK2_ENCODER_SCIBERT_MLFLOW_MODEL_URI="runs:/<run_id>/hf_model"
TASK2_ENCODER_THRESHOLD=0.5
```

**Nota (S3 directo, sin servidor MLflow):**

TambiÃ©n puedes apuntar directamente a un prefix en S3, por ejemplo:
```bash
TASK2_ENCODER_ROBERTA_MLFLOW_MODEL_URI="s3://<bucket>/<prefix>/hf_model"
```
El backend soporta descarga desde S3 sin listar el bucket (Ãºtil si tu IAM tiene `Deny` para `s3:ListBucket` pero permite `s3:GetObject`).

**Scripts Ãºtiles:**
- Subir una carpeta HF a MLflow: `app/backend/scripts/mlflow_log_hf_model.py`

### Dependencias de la app

```
flask==3.0.3
flask-cors==4.0.1
flasgger==0.9.7.1
python-dotenv==1.0.1
```

Frontend: HTML + CSS + JavaScript vanilla (sin frameworks, sin build step).

---

## Despliegue en EC2 (Ubuntu) â€” guÃ­a rÃ¡pida

1) Instalar dependencias del sistema:
```bash
sudo apt-get update
sudo apt-get install -y python3 python3-venv python3-pip
```

2) Clonar repo y crear venv:
```bash
git clone <tu_repo>
cd <tu_repo>/app/backend
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

3) Instalar dependencias de inferencia (si usas `model="encoder"`):
```bash
pip install torch transformers safetensors
```

4) Configurar `.env` en la raÃ­z del repo (o exportar env vars). Para S3, idealmente usa un IAM Role en la instancia.

5) Ejecutar (dev):
```bash
python main.py
```

Para producciÃ³n, usa un WSGI server:
```bash
pip install gunicorn
gunicorn -w 2 -b 0.0.0.0:5000 main:app
```

## Integrantes

- Ãlvaro AndrÃ©s Ruiz FlÃ³rez
- JosÃ© David Yepes Tumay
- AndrÃ©s JuliÃ¡n GonzÃ¡lez Barrera
