# Microproyecto MAIA â€” AnÃ¡lisis de documentos cientÃ­ficos en espaÃ±ol

**Tema 1 â€“ 2026** Â· Grupo FLAG-TICsW Â· Universidad de los Andes

Este repositorio corresponde a un microproyecto MAIA cuyo objetivo es desarrollar una soluciÃ³n computacional para el anÃ¡lisis automÃ¡tico de documentos cientÃ­ficos en espaÃ±ol, abordando (i) la segmentaciÃ³n y clasificaciÃ³n retÃ³rica y (ii) la extracciÃ³n de contribuciones cientÃ­ficas.

## PropÃ³sito del repositorio

Este proyecto implementa:

- PreparaciÃ³n y curadurÃ­a de un corpus cientÃ­fico en espaÃ±ol.
- Modelos para segmentaciÃ³n y clasificaciÃ³n retÃ³rica.
- Modelos para detecciÃ³n de contribuciones cientÃ­ficas.
- EvaluaciÃ³n comparativa entre modelos entrenados y modelos de lenguaje.
- AplicaciÃ³n web interactiva para visualizaciÃ³n de resultados.
- Scripts reproducibles para experimentaciÃ³n y anÃ¡lisis de resultados.

El enfoque es acadÃ©mico y experimental, orientado a entregar una soluciÃ³n funcional y evaluable.

## Tareas

| Tarea | DescripciÃ³n | Labels / Tipos |
|-------|-------------|----------------|
| **Tarea 1** | SegmentaciÃ³n retÃ³rica | INTRO, BACK, METH, RES, DISC, CONTR, LIM, CONC |
| **Tarea 2** | ExtracciÃ³n de contribuciones | MetodolÃ³gica, EmpÃ­rica, Recurso, Conceptual |

## Estructura del proyecto

```
.
â”œâ”€â”€ app/                           # ğŸ–¥ï¸ AplicaciÃ³n web (frontend + backend)
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ main.py                # Entry point Flask
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py        # POST /api/analyze
â”‚   â”‚   â”‚   â””â”€â”€ comparison.py      # GET /api/compare/<id>
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ segmentation.py    # Tarea 1 (mock â†’ real)
â”‚   â”‚   â”‚   â”œâ”€â”€ contributions.py   # Tarea 2 (mock â†’ real)
â”‚   â”‚   â”‚   â””â”€â”€ models.py          # ConfiguraciÃ³n de modelos
â”‚   â”‚   â”œâ”€â”€ mock_data/             # JSONs de referencia
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ frontend/
â”‚       â”œâ”€â”€ index.html             # Vista 1: Entrada de texto
â”‚       â”œâ”€â”€ segmentation.html      # Vista 2: SegmentaciÃ³n retÃ³rica
â”‚       â”œâ”€â”€ contributions.html     # Vista 3: Contribuciones
â”‚       â”œâ”€â”€ comparison.html        # Vista 4: ComparaciÃ³n de modelos
â”‚       â”œâ”€â”€ css/styles.css
â”‚       â””â”€â”€ js/
â”‚           â”œâ”€â”€ app.js
â”‚           â”œâ”€â”€ api.js
â”‚           â””â”€â”€ charts.js
â”‚
â”œâ”€â”€ datos/                         # ğŸ“Š Datos del proyecto
â”‚   â””â”€â”€ core/                      # Corpus cientÃ­fico crudo (CORE)
â”‚
â”œâ”€â”€ src/                           # ğŸ”§ CÃ³digo fuente principal
â”‚   â”œâ”€â”€ preprocessing/             # Limpieza, normalizaciÃ³n y segmentaciÃ³n
â”‚   â”œâ”€â”€ task1_rhetorical/          # SegmentaciÃ³n y clasificaciÃ³n retÃ³rica
â”‚   â”œâ”€â”€ task2_contributions/       # ExtracciÃ³n de contribuciones cientÃ­ficas
â”‚   â”œâ”€â”€ models/                    # DefiniciÃ³n y carga de modelos
â”‚   â””â”€â”€ utils/                     # Funciones auxiliares comunes
â”‚
â”œâ”€â”€ experiments/                   # ğŸ§ª Experimentos y configuraciones
â”‚   â”œâ”€â”€ task1/                     # Experimentos de clasificaciÃ³n retÃ³rica
â”‚   â””â”€â”€ task2/                     # Experimentos de extracciÃ³n de contribuciones
â”‚
â”œâ”€â”€ evaluation/                    # ğŸ“ˆ EvaluaciÃ³n y anÃ¡lisis de resultados
â”‚   â”œâ”€â”€ metrics/                   # MÃ©tricas cuantitativas
â”‚   â””â”€â”€ error_analysis/            # AnÃ¡lisis cualitativo de errores
â”‚
â”œâ”€â”€ notebooks/                     # ğŸ““ AnÃ¡lisis exploratorio y pruebas
â”œâ”€â”€ artifacts/                     # Artefactos generados
â”œâ”€â”€ configs/                       # Configuraciones de modelos y experimentos
â”œâ”€â”€ data_lake/scripts/             # Scripts de data lake
â”‚
â”œâ”€â”€ .dvc/                          # ConfiguraciÃ³n DVC
â”œâ”€â”€ .dvcignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ datos.dvc                      # Tracking DVC del corpus
â”œâ”€â”€ Propuesta_Proyecto_PLN_FLAG.pdf
â””â”€â”€ README.md
```

## DescripciÃ³n de componentes

| Carpeta | DescripciÃ³n |
|---------|-------------|
| `app/` | AplicaciÃ³n web con backend Flask y frontend vanilla. Interfaz para analizar textos y comparar modelos. |
| `datos/` | Corpus de documentos cientÃ­ficos en espaÃ±ol (CORE API). |
| `src/` | LÃ³gica central: preprocesamiento, clasificaciÃ³n retÃ³rica, detecciÃ³n de contribuciones. |
| `experiments/` | Scripts para ejecutar experimentos controlados y comparables. |
| `evaluation/` | CÃ¡lculo de mÃ©tricas, matrices de confusiÃ³n y anÃ¡lisis de errores. |
| `notebooks/` | ExploraciÃ³n de datos, pruebas de modelos y anÃ¡lisis intermedios. |

## Alcance

Este repositorio cubre todo el flujo de soluciÃ³n:

- Desde documentos cientÃ­ficos crudos â†’ hasta resultados evaluados y comparables
- Incluye una aplicaciÃ³n web para visualizaciÃ³n interactiva de ambas tareas

---

## CÃ³mo lanzar el proyecto

### 1. Configurar AWS (credenciales)

```bash
aws configure
```

Verificar que quedaron activas:

```bash
aws sts get-caller-identity
```

### 2. Descargar los datos con DVC

Desde la raÃ­z del repositorio:

```bash
dvc pull
```

### 3. Ejecutar la aplicaciÃ³n web

```bash
# Desde la raÃ­z del repositorio
cd app/backend

# Crear entorno virtual (recomendado)
python -m venv venv
source venv/bin/activate      # macOS/Linux
# venv\Scripts\activate       # Windows

# Instalar dependencias
pip install -r requirements.txt

# Ejecutar el servidor
python main.py
# â†’ Servidor en http://localhost:5000
```

Abre http://localhost:5000 en tu navegador.

---

## AplicaciÃ³n web â€” SciText-ES

### Vistas

1. **Entrada** (`/`) â€” Pega el texto, selecciona modelo y tareas
2. **SegmentaciÃ³n** (`/segmentation.html`) â€” PÃ¡rrafos clasificados con colores y confianza
3. **Contribuciones** (`/contributions.html`) â€” Fragmentos con aportes identificados
4. **ComparaciÃ³n** (`/comparison.html`) â€” MÃ©tricas F1/PrecisiÃ³n/Recall/Latencia de los 3 modelos

### API

**`POST /api/analyze`**
```json
{
  "text": "Texto del artÃ­culoâ€¦",
  "model": "encoder | llm | api",
  "tasks": ["segmentation", "contributions"]
}
```
Devuelve segmentos etiquetados y fragmentos con contribuciones.

**`GET /api/compare/<analysis_id>`**

Devuelve mÃ©tricas comparativas de los 3 modelos para el texto analizado.

### CÃ³mo reemplazar los mocks por modelos reales

Los servicios estÃ¡n diseÃ±ados para facilitar la transiciÃ³n:

**Tarea 1 â€” `app/backend/services/segmentation.py`**
```python
def analyze_segments(text: str, model: str) -> dict:
    # Reemplaza _mock_analyze() por _call_real_model()
    return _call_real_model(text, model)  # â† descomentar cuando estÃ© listo
```

**Tarea 2 â€” `app/backend/services/contributions.py`**
```python
def analyze_contributions(segments: list[dict], model: str) -> dict:
    # Mismo patrÃ³n
    return _call_real_model(segments, model)
```

Los stubs tienen comentarios con ejemplos de integraciÃ³n para Hugging Face (encoder), Ollama (LLM open-weight) y OpenAI SDK (API comercial).

### Variables de entorno

```bash
PORT=5000       # Puerto del servidor (por defecto: 5000)
DEBUG=1         # Modo debug de Flask (por defecto: 1)
```

### Dependencias de la app

```
flask==3.0.3
flask-cors==4.0.1
```

Frontend: HTML + CSS + JavaScript vanilla (sin frameworks, sin build step).

---

## Integrantes

- Ãlvaro AndrÃ©s Ruiz FlÃ³rez
- JosÃ© David Yepes Tumay
- AndrÃ©s JuliÃ¡n GonzÃ¡lez Barrera
